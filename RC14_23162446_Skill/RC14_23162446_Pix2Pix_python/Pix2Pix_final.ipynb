{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "N8yDV3ItwdnY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models # add models to the list\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o24rpjhjwMvO",
    "outputId": "c371c3d4-72bc-4dbf-9b29-0a8133673754"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f3c50e9d90>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hb6BWPIjjrBa",
    "outputId": "30d59a62-110f-448e-d7a3-611818765fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "VsGIDlKfwgMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "TRAIN_DIR = \"data/train\"\n",
    "VAL_DIR = \"data/val\"\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 20 #Use a number that can divide the total of photos- (here 961)\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS_IMG = 3\n",
    "L1_LAMBDA = 100\n",
    "LAMBDA_GP = 10\n",
    "NUM_EPOCHS = 40  #we suggest to start with 5\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_DISC = \"disc.pth.tar\"\n",
    "CHECKPOINT_GEN = \"gen.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Xc9uu4LswkkV"
   },
   "outputs": [],
   "source": [
    "#Discriminator model for Pix2Pix\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2,features[0],kernel_size=4,stride=2,padding=1,padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        x = self.initial(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Jm7rc3ANwvpN"
   },
   "outputs": [],
   "source": [
    "#Generator model for Pix2Pix\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down2 = Block(features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down3 = Block(features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down4 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down5 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down6 = Block(features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features * 8, features * 8, 4, 2, 1,padding_mode=\"reflect\"), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up2 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up3 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up4 = Block(features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.up5 = Block(features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.up6 = Block(features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        d7 = self.down6(d6)\n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        up1 = self.up1(bottleneck)\n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
    "        return self.final_up(torch.cat([up7, d1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "aPL878jTwy2m"
   },
   "outputs": [],
   "source": [
    "#Train function for Pix2Pix\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler,):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_fake = gen(x)\n",
    "            D_real = disc(x, y)\n",
    "            D_fake = disc(x, y_fake.detach())\n",
    "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
    "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
    "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train generator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            D_fake = disc(x, y_fake)\n",
    "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
    "            L1 = l1_loss(y_fake, y) * L1_LAMBDA\n",
    "            G_loss = G_fake_loss + L1\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            loop.set_postfix(\n",
    "                D_real=torch.sigmoid(D_real).mean().item(),\n",
    "                D_fake=torch.sigmoid(D_fake).mean().item(),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "h_7H3bXls8bR"
   },
   "outputs": [],
   "source": [
    "# Utils for Pix2Pix\n",
    "\n",
    "def save_some_examples(gen, val_loader, epoch, folder):\n",
    "  ##ADDITION\n",
    "    y_pred = []\n",
    "  ##\n",
    "\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "\n",
    "        ##ADDITION\n",
    "        y_prediction = y_fake.cpu().numpy() #convert pytorch tensor to numpy\n",
    "        y_pred.append(y_prediction)\n",
    "        ##\n",
    "\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        if epoch == 1:\n",
    "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "    gen.train()\n",
    "\n",
    "    ##ADDITION\n",
    "    ypred = np.array(y_pred)\n",
    "    #print(ypred)\n",
    "    np.save(\"ypred.npy\", ypred)\n",
    "    ##\n",
    "\n",
    "def save_some_examples_modified(gen, val_loader, epoch, folder, iteration):\n",
    "  ##ADDITION\n",
    "    y_pred = []\n",
    "  ##\n",
    "    \n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "\n",
    "        ##ADDITION\n",
    "        y_prediction = y_fake.cpu().numpy() #convert pytorch tensor to numpy\n",
    "        y_pred.append(y_prediction)\n",
    "        ##\n",
    "\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}_{i}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}_{i}.png\")\n",
    "        if epoch == 1:\n",
    "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}_{i}.png\")\n",
    "    gen.train()\n",
    "\n",
    "    ##ADDITION\n",
    "    ypred = np.array(y_pred)\n",
    "    #print(ypred)\n",
    "    np.save(f\"ypred_{i}.npy\", ypred)\n",
    "    ##\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=config.DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9di90-PxTdK"
   },
   "source": [
    "### SUPERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JtAouWt5J2L"
   },
   "source": [
    "**Create the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OUrdMmGJxWZs",
    "outputId": "7e497c46-2285-410e-c55f-3967a4a69d2e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SpaceSyntax_Choice_50m_None</th>\n",
       "      <th>Supermarkets_50m_id_count</th>\n",
       "      <th>noise final50m-sum_noise sum</th>\n",
       "      <th>BuilingArea_50m_None</th>\n",
       "      <th>overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>311.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2378.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2501.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>689.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  SpaceSyntax_Choice_50m_None  Supermarkets_50m_id_count  \\\n",
       "0   1                          NaN                        NaN   \n",
       "1   2                          NaN                        NaN   \n",
       "2   3                          NaN                        NaN   \n",
       "3   4                          NaN                        NaN   \n",
       "4   5                          NaN                        NaN   \n",
       "\n",
       "   noise final50m-sum_noise sum  BuilingArea_50m_None  overlap  \n",
       "0                           NaN                 311.0      NaN  \n",
       "1                           NaN                 101.0      NaN  \n",
       "2                           NaN                2378.0      NaN  \n",
       "3                           NaN                2501.0      NaN  \n",
       "4                           NaN                 689.0      NaN  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "df = pd.read_csv('50m 3.12.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D78hwVt422rj",
    "outputId": "b3b55a67-d4c5-4673-e794-6bcf04278dee",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'SpaceSyntax_Choice_50m_None', 'Supermarkets_50m_id_count',\n",
       "       'noise final50m-sum_noise sum', 'BuilingArea_50m_None', 'overlap'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4fADhDIxaL2",
    "outputId": "dc84892b-a3fd-4ff0-9c66-57d0e8b7c99e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 4)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform to numpy\n",
    "data=np.array(df[['id','SpaceSyntax_Choice_50m_None', 'Supermarkets_50m_id_count',\n",
    "       'noise final50m-sum_noise sum']])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because of the id system in python , width and height are opposite to QGIS\n",
    "w=400 #width\n",
    "h=375 #height\n",
    "s=256 #size\n",
    "step=32 #step\n",
    "cutoff=400 # how much overlap with the test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PwKk3T1iGs1t",
    "outputId": "2ff4c7a4-3b2e-4eef-86ae-d485e9bbe28a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 375, 400)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.transpose(data)\n",
    "\n",
    "data= data.reshape(4,h,w)\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into set\n",
    "\n",
    "idlist=[]\n",
    "\n",
    "for j in range(s//2-1,w-s//2,step):\n",
    "    for i in range(s//2-1,h-s//2,step):\n",
    "        idlist.append(data[0,i,j])\n",
    "\n",
    "\n",
    "dataclip=np.empty((0,3,256,256))\n",
    "\n",
    "for id in idlist:\n",
    "    #print(id)\n",
    "    \n",
    "    row=int(id//w) #UPDATE\n",
    "    #print(row)\n",
    "\n",
    "    col=int(id-w*row -1) #UPDATE\n",
    "    #print(col)\n",
    "    \n",
    "    #print(s)\n",
    "    minRow = row-s//2+1\n",
    "    maxRow = row+s//2+1\n",
    "    minCol = col-s//2+1\n",
    "    maxCol = col+s//2+1\n",
    "\n",
    "    #remove id column because we do not need it further\n",
    "    sample= data[1:,minRow:maxRow,minCol:maxCol]\n",
    "\n",
    "    #print(sample.shape)\n",
    "    dataclip=np.append(dataclip,[sample],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgQygcnza8PD",
    "outputId": "3cff38ab-8285-4167-cfad-2e749ef5371f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3, 256, 256)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataclip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "\n",
    "#Normalize R\n",
    "dataclip[:,0,:,:] = dataclip[:,0,:,:]/np.max(dataclip[:,0,:,:]) \n",
    "\n",
    "#Normalize G\n",
    "dataclip[:,1,:,:] = dataclip[:,1,:,:]/np.max(dataclip[:,1,:,:]) \n",
    "\n",
    "#Normalize B\n",
    "dataclip[:,2,:,:] = dataclip[:,2,:,:]/np.max(dataclip[:,2,:,:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-2u5pvXG31g"
   },
   "source": [
    "**Repeat the process for labels of training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCvLP4J3H4vE",
    "outputId": "51f06412-05e2-4c90-b472-5d64a733121d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 2)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform to numpy\n",
    "data=np.array(df[['id', 'BuilingArea_50m_None']])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoTXrvu7H-ZF",
    "outputId": "4c9c88c5-1791-4859-fc63-0f2507de9116"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 375, 400)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.transpose(data)\n",
    "\n",
    "data= data.reshape(2,h,w)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "-O3jyH0cPj6b"
   },
   "outputs": [],
   "source": [
    "ylabel=np.empty((0,1,256,256))\n",
    "\n",
    "for id in idlist:\n",
    "    #print(id)\n",
    "\n",
    "    row=int(id//w)#UPDATE\n",
    "    #print(row)\n",
    "\n",
    "    col=int(id-w*row -1)#UPDATE\n",
    "    #print(col)\n",
    "\n",
    "    #print(s)\n",
    "    minRow = row-s//2+1\n",
    "    maxRow = row+s//2+1\n",
    "    minCol = col-s//2+1\n",
    "    maxCol = col+s//2+1\n",
    "\n",
    "    #remove id column because we do not need it further\n",
    "    sample= data[1:,minRow:maxRow,minCol:maxCol]\n",
    "\n",
    "    #print(sample.shape)\n",
    "    ylabel=np.append(ylabel,[sample],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJir1dK8aAsm",
    "outputId": "0c399630-7244-4755-80a9-be1f875bd432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1, 256, 256)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ylabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "ylabel[:,0,:,:] = ylabel[:,0,:,:]/np.max(ylabel[:,0,:,:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE THE OVERLAPING IMAGES WITH TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform to numpy\n",
    "data=np.array(df[['id','overlap']])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 375, 400)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.transpose(data)\n",
    "\n",
    "data= data.reshape(2,h,w)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaplabel=np.empty((0,1,256,256))\n",
    "\n",
    "for id in idlist:\n",
    "    #print(id)\n",
    "\n",
    "    row=int(id//w) #UPDATE\n",
    "    #print(row)\n",
    "\n",
    "    col=int(id-w*row -1) #UPDATE\n",
    "    #print(col)\n",
    "\n",
    "    #print(s)\n",
    "    minRow = row-s//2+1\n",
    "    maxRow = row+s//2+1\n",
    "    minCol = col-s//2+1\n",
    "    maxCol = col+s//2+1\n",
    "\n",
    "    #remove id column because we do not need it further\n",
    "    sample= data[1:,minRow:maxRow,minCol:maxCol]\n",
    "\n",
    "    #print(sample.shape)\n",
    "    \n",
    "    overlaplabel=np.append(overlaplabel,[sample],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the sum of Overlap for each image \n",
    "overlaplabelSum=np.sum(np.sum(overlaplabel[:,0],axis=1),axis=1)\n",
    "\n",
    "#Select the images that only have a small overlap \n",
    "dataclipNew=dataclip[np.where(overlaplabelSum<cutoff)]\n",
    "ylabelNew=ylabel[np.where(overlaplabelSum<cutoff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 256, 256)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check shape of X data train\n",
    "dataclipNew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 256, 256)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recreating 3 channels in total\n",
    "rgb = np.hstack((ylabelNew,ylabelNew,ylabelNew))\n",
    "rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNo7-cJft3Pd"
   },
   "source": [
    "### Create the test set / validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "M4bxfVddt5m6",
    "outputId": "19a6f0c0-2a07-4a1d-92f4-c5d717428f39",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SpaceSyntax_10m_choice_ASA216_T10</th>\n",
       "      <th>Supermarkets_10m (1)_id_count</th>\n",
       "      <th>noise sum_10m_NoiseClass10m</th>\n",
       "      <th>BuilingArea_10m_None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.5</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.5</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.5</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5946.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.5</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5946.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  SpaceSyntax_10m_choice_ASA216_T10  Supermarkets_10m (1)_id_count  \\\n",
       "0   1                                NaN                            NaN   \n",
       "1   2                                NaN                            NaN   \n",
       "2   3                                NaN                            NaN   \n",
       "3   4                             5946.0                            NaN   \n",
       "4   5                             5946.0                            NaN   \n",
       "\n",
       "   noise sum_10m_NoiseClass10m  BuilingArea_10m_None  \n",
       "0                         57.5                  72.0  \n",
       "1                         57.5                  39.0  \n",
       "2                         57.5                  87.0  \n",
       "3                         57.5                  26.0  \n",
       "4                          NaN                   1.0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "dft = pd.read_csv('10m 3.12.csv')\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iBR7r2-u9_t",
    "outputId": "751fc7f6-da76-43ed-ed32-6657fdec7fe1",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'SpaceSyntax_10m_choice_ASA216_T10',\n",
       "       'Supermarkets_10m (1)_id_count', 'noise sum_10m_NoiseClass10m',\n",
       "       'BuilingArea_10m_None'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dESP7maSvCVl",
    "outputId": "6fc7fd2a-8f84-49d6-a05a-122e0c77b19d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300, 300)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform to numpy\n",
    "datatest=np.array(dft[['id', 'SpaceSyntax_10m_choice_ASA216_T10',\n",
    "       'Supermarkets_10m (1)_id_count', 'noise sum_10m_NoiseClass10m']])\n",
    "datatest.shape\n",
    "\n",
    "datatest=np.transpose(datatest)\n",
    "\n",
    "datatest= datatest.reshape(4,300,300)\n",
    "datatest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXHFtbeivbbx",
    "outputId": "cfc80f16-1ae3-4053-af9b-edc4023bdf3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38228.0, 38260.0, 47828.0, 47860.0]\n"
     ]
    }
   ],
   "source": [
    "#Divide into set\n",
    "\n",
    "w=300 #width\n",
    "h=300 #height\n",
    "s=256 #size\n",
    "step=32 #step\n",
    "\n",
    "\n",
    "idlisttest=[]\n",
    "\n",
    "for i in range(s//2-1,w-s//2,step):\n",
    "    for j in range(s//2-1,h-s//2,step):\n",
    "        idlisttest.append(datatest[0,i,j])\n",
    "\n",
    "print(idlisttest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "qcruCGIIv97A"
   },
   "outputs": [],
   "source": [
    "datacliptest=np.empty((0,3,256,256))\n",
    "##UPDATE\n",
    "idvaltest=np.empty((0,256,256))\n",
    "\n",
    "for id in idlisttest:\n",
    "    #print(id)\n",
    "\n",
    "    row=int(id//h)\n",
    "    #print(row)\n",
    "\n",
    "    col=int(id-h*row -1)\n",
    "    #print(col)\n",
    "\n",
    "    #print(s)\n",
    "    minRow = row-s//2+1\n",
    "    maxRow = row+s//2+1\n",
    "    minCol = col-s//2+1\n",
    "    maxCol = col+s//2+1\n",
    "\n",
    "    #remove id column because we do not need it further\n",
    "    sample= datatest[1:,minRow:maxRow,minCol:maxCol]\n",
    "    idval=datatest[0,minRow:maxRow,minCol:maxCol]\n",
    "\n",
    "    #print(sample.shape)\n",
    "    datacliptest=np.append(datacliptest,[sample],axis=0)\n",
    "    idvaltest=np.append(idvaltest,[idval],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SF-ZQipv6Gh",
    "outputId": "658d2098-d8c1-4932-f8c7-71709917ecf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 256, 256)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacliptest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "\n",
    "#Normalize R\n",
    "datacliptest[:,0,:,:] = datacliptest[:,0,:,:]/np.max(datacliptest[:,0,:,:]) \n",
    "\n",
    "#Normalize G\n",
    "datacliptest[:,1,:,:] = datacliptest[:,1,:,:]/np.max(datacliptest[:,1,:,:]) \n",
    "\n",
    "#Normalize B\n",
    "datacliptest[:,2,:,:] = datacliptest[:,2,:,:]/np.max(datacliptest[:,2,:,:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu2UbtuLp1pE"
   },
   "source": [
    "**TRAIN THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "RQXT-vaRxIvq"
   },
   "outputs": [],
   "source": [
    "#Configurations\n",
    "\n",
    "disc = Discriminator(in_channels=3).to(DEVICE)\n",
    "gen = Generator(in_channels=3, features=64).to(DEVICE)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "BCE = nn.BCEWithLogitsLoss()\n",
    "L1_LOSS = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImFiZDRwxNJX",
    "outputId": "e90c04a1-f1ba-4dd3-93b7-a06d07dfc0b8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.97s/it, D_fake=0.533, D_real=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.59s/it, D_fake=0.483, D_real=0.585]\n",
      "100%|█████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.95s/it, D_fake=0.437, D_real=0.56]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.08s/it, D_fake=0.406, D_real=0.538]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.06s/it, D_fake=0.421, D_real=0.541]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.11s/it, D_fake=0.407, D_real=0.583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.85s/it, D_fake=0.383, D_real=0.609]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.87s/it, D_fake=0.359, D_real=0.615]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.18s/it, D_fake=0.376, D_real=0.627]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.20s/it, D_fake=0.348, D_real=0.646]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.79s/it, D_fake=0.396, D_real=0.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.16s/it, D_fake=0.317, D_real=0.666]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.16s/it, D_fake=0.293, D_real=0.639]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.24s/it, D_fake=0.304, D_real=0.645]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.73s/it, D_fake=0.345, D_real=0.694]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.02s/it, D_fake=0.295, D_real=0.678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.13s/it, D_fake=0.26, D_real=0.707]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.12s/it, D_fake=0.255, D_real=0.686]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.13s/it, D_fake=0.245, D_real=0.704]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.17s/it, D_fake=0.257, D_real=0.708]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.10s/it, D_fake=0.227, D_real=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.95s/it, D_fake=0.231, D_real=0.725]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.22s/it, D_fake=0.253, D_real=0.727]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.03s/it, D_fake=0.215, D_real=0.756]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.16s/it, D_fake=0.239, D_real=0.737]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.31s/it, D_fake=0.219, D_real=0.744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.05s/it, D_fake=0.274, D_real=0.703]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.10s/it, D_fake=0.424, D_real=0.744]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.90s/it, D_fake=0.186, D_real=0.754]\n",
      "100%|█████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.91s/it, D_fake=0.28, D_real=0.591]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.27s/it, D_fake=0.368, D_real=0.775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.09s/it, D_fake=0.201, D_real=0.78]\n",
      "100%|█████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.95s/it, D_fake=0.233, D_real=0.73]\n",
      "100%|█████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.22s/it, D_fake=0.201, D_real=0.69]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.11s/it, D_fake=0.294, D_real=0.761]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.02s/it, D_fake=0.249, D_real=0.756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.20s/it, D_fake=0.249, D_real=0.671]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.02s/it, D_fake=0.227, D_real=0.751]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.54s/it, D_fake=0.207, D_real=0.791]\n",
      "100%|████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.96s/it, D_fake=0.195, D_real=0.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "##ADDITION\n",
    "tensor_x = torch.Tensor(dataclipNew) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(rgb)\n",
    "\n",
    "train_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "##\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS,)\n",
    "\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_fn(disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler)\n",
    "    \n",
    "    if SAVE_MODEL and epoch % 5 == 0:\n",
    "        save_checkpoint(gen, opt_gen, filename = CHECKPOINT_GEN)\n",
    "        save_checkpoint(disc, opt_disc, filename = CHECKPOINT_DISC)\n",
    "\n",
    "save_checkpoint(gen, opt_gen, filename = CHECKPOINT_GEN)\n",
    "save_checkpoint(disc, opt_disc, filename = CHECKPOINT_DISC)\n",
    "        \n",
    "# Ideally we need D_fake=0 and D_real=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 创建名为 'final' 的文件夹在桌面上\n",
    "desktop_path = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop')\n",
    "final_folder_path = os.path.join(desktop_path, 'final')\n",
    "if not os.path.exists(final_folder_path):\n",
    "    os.makedirs(final_folder_path)\n",
    "\n",
    "# 循环遍历数据\n",
    "for i in range(len(datacliptest)):\n",
    "    tensor_x = torch.Tensor(datacliptest[i].reshape(1, 3, 256, 256))  # 转换为 torch 张量\n",
    "    # 我们可以使用一个假的测试目标/我们只需要它作为占位符\n",
    "    testRandomY = np.empty_like(datacliptest[i].reshape(1, 3, 256, 256))\n",
    "    tensor_y = torch.Tensor(testRandomY)\n",
    "\n",
    "    val_dataset = TensorDataset(tensor_x, tensor_y)  # 创建数据集\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # 保存文件到 'final' 文件夹中\n",
    "    save_some_examples_modified(gen, val_loader, 5, folder=final_folder_path, iteration=i)  # 记得选择 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egRQpSBt18Xo"
   },
   "source": [
    "**SAVE THE RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamples = len(datacliptest)\n",
    "yFull = np.zeros((nSamples,3,256,256))\n",
    "\n",
    "for i in range(len(datacliptest)): \n",
    "    yFull[i] = np.load(f\"ypred_{i}.npy\")\n",
    "    \n",
    "yFull=yFull.reshape(1, 3, -1)\n",
    "yFull.shape\n",
    "idvaltest2=idvaltest.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "eGFAD6g92HUg",
    "outputId": "3e1b574b-251c-49c2-a8a8-1a976b8cd68f"
   },
   "outputs": [],
   "source": [
    "dfy=pd.DataFrame(idvaltest2[:],columns=['id'])\n",
    "dfy['ch1']= yFull[0,0,:]\n",
    "dfy['ch2']= yFull[0,1,:]\n",
    "dfy['ch3']= yFull[0,2,:]\n",
    "dfy.to_csv(f'./_pred_LG.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
